<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Nicolas Slotte" />


<title>Coffee Beans Classification</title>

<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="justify.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Nicolas Slotte</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/NicolasSlotte">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://www.linkedin.com/in/nicolas-slotte-2065551b8/">
    <span class="fab fa-linkedin fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Coffee Beans Classification</h1>
<h3 class="subtitle">An attempt to image acquisition, classification and processing using RVision</h3>
<h4 class="author">Nicolas Slotte</h4>
<h4 class="date">First Iteration | February - May 2024</h4>

</div>


<pre class="r"><code># Initiate

rm(list = ls())

# Packages

library(bookdown)
library(tidyverse)
library(Rvision)
library(stringr)
library(caret)
library(kableExtra)</code></pre>
<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>For coffee nerds, the investigation of various coffee tastes using different brewing techniques and bean types can be an exciting hobby. In some cases, the differences in bean aesthetics between some coffee types can be easily spotted at first glance. As in some cases we are able to quickly segregate beans based solely on their aesthetics, the main question driving project is the following: Based on a selection of bean images, could we build a program capable of predicting a bean type as accurately as we can? This document summarizes the development of an image processing tool in R designed to perform an automatic image classification of two different types of coffee beans.</p>
</div>
<div id="methods" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Methods</h1>
<div id="motivation-and-definitions" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Motivation and definitions</h2>
<p>Our motivation is the following: Based on bean aesthetics predictors extracted from a ensemble of images, could we build a statistical model capable of accurately predicting a bean type. To address this question, we started simple by selecting two groups of beans with distinct visual features. The first feature being the size (wider and narrower beans) and the second being the color (darker and lighter beans). As the purpose of this project was not to use dimension reduction techniques, we established that this variable selection was a fair assumption to start with.</p>
<p>On this basis, our goal is to build a supervised model that takes a feature as input and returns a prediction of the outcome when we don’t know the outcome. Our approach will be to (i) build a reliable dataset (ii) train a model using a labeled data partition (iii) apply this model on an unlabeled data partition to make prediction and then (iv) assess the model accuracy by comparing the predictions and the actual outcomes retrieved from the unlabeled data partition.</p>
<p>We will use <span class="math inline">\(Y\)</span> to denote the outcome (group 1, group 2) and <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> to denote the features (size and color). In this case, the outcome <span class="math inline">\(Y\)</span> is categorical and the <span class="math inline">\(K\)</span> classes can be denoted <span class="math inline">\(k = 0, 1\)</span>.</p>
<p>Hence, our conditional probability for each class <span class="math inline">\(k\)</span> can be defined as</p>
<p><span class="math display">\[
Pr(Y = k \ | \ X_1 = x_1, X_2 = x_2), for \ k = 0, 1 \tag {1}
\]</span></p>
<p>A total of 200 coffee beans where gathered. The first group consists of a hundred beans of Douwe Egberts Dessert (hereafter “DED”) purchased in a supermarket, and the second group consists of a hundred beans of Ethiopia GUJI Natural (hereafter “EGN”) purchased in a fancy coffee shop. As we suspect one of the predictors will be the size, we took care to homogenize each batch in order to minimize a granulometry based fractionation before sampling. In figure 2.1, we can see that the DED beans seem larger and darker than most of the EGN beans.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="img/IMG_20240211_193848_2.jpg" alt="DED beans (left) and EGN beans (right) sampled." width="50%" />
<p class="caption">
Figure 2.1: DED beans (left) and EGN beans (right) sampled.
</p>
</div>
</div>
<div id="image-aquisition" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Image aquisition</h2>
<p>In order to obtain a precise estimate of the size and color of each coffee bean, the first step consisted in getting individual high resolution images of the sampled beans. No matter how sophisticated our future model can be, if we feed it mediocre data, the predictions will be sketchy at best.</p>
<p>To obtain higher quality images, we built a setup using a magnitude 10 NEOC binocular (c.f. Figure 2.2) wired to an unbranded 1080p USB camera mounted onto a tripod. We selected one unique DED bean to fine tune the resolution, light intensity and camera alignment using a rule of thumb. As we are looking for differences between bean types, the optimal tuning parameters would be the ones that maximizes those optical differences. For instance, the best lighting tuning would resemble an bell shaped optimization function where the optimal intensity would lie between (i) too bright so that we have a ceiling effect that reduce the variability in the clear range (ii) too dark so that we have a floor effect that reduce the variability in the dark range. As we have no prior knowledge regarding the color range of maximum variability between bean types, our first optical tuning is essentially a rough visual appreciation. Once calibrated, the optics and lighting specifications were kept constant in order to avoid adding a systematic bias to the images. Each bean was placed individually under the binocular using tweezers for image acquisition.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="img/IMG_20240320_170555.jpg" alt="NEOC binocular used for magnification." width="30%" />
<p class="caption">
Figure 2.2: NEOC binocular used for magnification.
</p>
</div>
</div>
<div id="image-processing" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Image processing</h2>
<p>Once the bean images were successfully rendered, the next step consisted in cleaning those images. The approach we used was the following: 1. Import all the images into an R environment 2. Convert the images into R objects and reduce their dimension to work with lighter objects 3. Isolate the bean from the background, since only ~ 20% of each image contains the actual bean.</p>
<div id="importation" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Importation</h3>
<p>We imported all the images names stored into the root images folder (<em>file.names</em> variable). Once the <em>.jpg</em> extensions were removed from the files names and stored into a separate vector (<em>clean_names</em> variable), we looped through all the files names, loaded the files as images using the function <em>image</em> from the RVision package and assigned the cleaned names using the <em>assign</em> function. We also extracted the category of each image (DED or EGN) and stored the results into the vector <em>outcomes</em>. For instance, the figure 2.3 shows the first DED bean image loaded into our R environment.</p>
<pre class="r"><code># extract the file names, images names and categories

path &lt;- &quot;C:/Data/Privé/IdeasForScienceProjects/Introduction to Machine Learning/grains/&quot;

file.names &lt;- list.files(path = path, pattern=&quot;\\.jpg$&quot;, recursive = FALSE)
file.names &lt;- file.names[1:length(file.names)-1]
clean_names &lt;- str_sub(file.names, end = -5)
outcomes &lt;- str_split_i(clean_names, pattern = &quot;_&quot;, i = 1)

# load the images and assign their names

for (i in 1:length(file.names)) {
  
  name &lt;- file.names[i]
  full_dir &lt;- file.path(path, name)
  im &lt;- image(filename = full_dir)
  
  name_clean &lt;- str_sub(name, end = -5)
  name_clean
  assign(name_clean, im)
  
}</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-4"></span>
<img src="img/ded_1.jpg" alt="Image of the first DED bean." width="50%" />
<p class="caption">
Figure 2.3: Image of the first DED bean.
</p>
</div>
</div>
<div id="conversion" class="section level3" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Conversion</h3>
<p>A digital image consists of a gridded frame where each pixel stores an intensity value between 0 (minimum intensity value = black) and 255 (maximum intensity value = white). For instance, a black and white image of dimension 28x28 pixels will be stored in R as an array of dimension 28 28 1. On the other hand, colored images are multichannel gridded frames, where each red, green and blue channel store a gridded frame which contains the intensity values for that specific color. Hence, for colored images, the dimensions of the R array is 28 28 3.</p>
<p>The dimensions of the images imported into our R environment are to large to perform fast manipulations. We reduced the overall dimension of each image going from 1080 1920 3 to 100 100 1. To achieve this transformation, we 1. stored all the images into one single R object to facilitate the data manipulation using a loop 2. defined the final dimension desired for our images (<em>dim</em> variable) 3. changed the color space to gray using the function <em>changeColorSpace</em> so that we can focus on only one color channel 4. cut the useless part of the images not containing the beans in order to have a square format 5. rescaled our 1080 1080 1 images to 100 100 1 using the function <em>resize</em> 6. converted our RVision images into matrices 7. stored the matrices into a array.</p>
<pre class="r"><code># stores the images into a list

x = list()
for (i in 1:length(clean_names)) {
  x[[i]] = get(clean_names[i])
}

# pixel reduction

dim &lt;- 100
for (i in 1:length(x)) {
  
  x[[i]] &lt;- changeColorSpace(x[[i]], &quot;GRAY&quot;) # gray
  x[[i]] &lt;- image(x[[i]][c(0:1080), c(500:1580) , ]) # cut
  x[[i]] &lt;- resize(x[[i]], dim, dim, interpolation = &quot;linear&quot;) # rescale
  x[[i]] &lt;- as.matrix(as.data.frame(as.matrix(x[[i]]))) # reduce matrix dimension
  
}

# fill an empty array with the matrices from the list

x_ &lt;- array(NA, dim = c(length(x), dim, dim))
for (i in 1:length(x)) {
  x_[i, , ] &lt;- x[[i]]
}</code></pre>
<p>The dimension of the array created is 200 100 100. 200 being the number of matrices and 100 100 being the number of lines and columns of each matrix. Now that the images are correctly imported and formatted as matrices, we can convert them into data frames and plot the corresponding images using ggplot.</p>
<pre class="r"><code># list the bean names for plot

list_for_plot &lt;- c(&quot;ded_1&quot;,
                   &quot;ded_2&quot;,
                   &quot;ded_3&quot;,
                   &quot;ded_4&quot;,
                   &quot;egn_1&quot;,
                   &quot;egn_2&quot;,
                   &quot;egn_3&quot;,
                   &quot;egn_4&quot;)

index &lt;- clean_names %in% list_for_plot
data &lt;- x_[index, , ]

# format the matrices into tidy data frames

gg_images &lt;- data.frame()
for (i in 1:dim(data)[1]) {
  
  image &lt;- as.data.frame(data[i, , ])
  colnames(image) &lt;- seq_len(ncol(image))
  image$y &lt;- seq_len(nrow(image))
  image &lt;- gather(image, &quot;x&quot;, &quot;value&quot;, -y)
  image$x &lt;- as.integer(image$x)
  image$image &lt;- list_for_plot[i]
  gg_images &lt;- rbind(gg_images, image)
  
}

# plot the data frames

ggplot(gg_images, aes(x = x, y = y, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = &quot;blue&quot;, high = &quot;orange&quot;, na.value = NA) +
  facet_wrap(image ~ ., ncol = 2) +
  theme_bw() +
  theme(panel.grid = element_blank())   +
  theme(aspect.ratio = 1)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-6"></span>
<img src="coffee_classification_files/figure-html/unnamed-chunk-6-1.png" alt="Overview of the first four matrices of each category. Colors suggested by O.Schmit" width="90%" />
<p class="caption">
Figure 2.4: Overview of the first four matrices of each category. Colors suggested by O.Schmit
</p>
</div>
</div>
<div id="cleaning" class="section level3" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Cleaning</h3>
<p>The most challenging part of this project lies in the image cleaning. From these images alone, the predictors (bean size and color) cannot be directly extracted as most of the images surface contains irrelevant information such as (i) the black contour caused by the camera misalignment (ii) the white background caused by the binocular light reflection on the sampling surface and (iii) the shadow of the bean caused by the lighting tilt (c.f. Figures 2.3 and 2.4).</p>
<p>In order to extract the beans from the images, we built a cleaning algorithm that consists in a multi-layered linear scanning loop that looks for breaking points in pixel intensity values, and replaces non-bean pixels with NA. For a defined image, the loop will select a column of pixels and verify how the difference in intensity value between a <span class="math inline">\(pixel_p\)</span> and the next pixel <span class="math inline">\(pixel_{p+1}\)</span> compares to a defined threshold. If the difference is lower than the threshold, we assume that it is not significant enough to be considered as a change in image domain. On the other hand, in case the difference is greater than the threshold, the position at which the difference is observed will be considered as a breaking point between two image domain, and the values observed prior to that point will be replaced by NA. Moreover, we added some features (e.g. addition of NAs filters as the more advanced layers work on depleted images; additional filters to refine the threshold detection; filtering layers etc.) in order to strengthen the process robustness to inter-domain intensity variability.</p>
<p>The first layers are designed to find the threshold between the black contour and the white background; The second layers are designed to find the threshold between the white contour and the bean; And a third and fourth layers were designed to roughly remove unused section of the images. Details can be found in the portion of code below.</p>
<p>To ease the cleaning, the shadows of the beans caused by the lighting tilt were kept as part of the beans in the first version of this algorithm. However, we do not expect it to introduce a significant bias due to its surface being directly proportional to the volume of the bean. Hence, it is likely that keeping that image portion for now will artificially increase the surface differences measured between beans.</p>
<p>For each layer, the threshold values were defined by manual trials and errors. Our final algorithm works for most beans images. However, for less than 5% of the images, some minor residual artefacts are observed after the cleaning, which correspond to isolated 1 pixel wide columns not properly NA-replaced. Since these artefacts are limited in pixel count and occurrence, they were considered tolerable and no further filtering was performed at this point.</p>
<p>Initially, the cleaning algorithm was executed remotely using the <em>script</em> function to simplify the code. But we embedded its contents into a loop for reporting purposes.</p>
<pre class="r"><code># empty array for clean matrices

x_clean &lt;- array(NA, dim = c(dim(x_)[1], dim, dim))
  
for (image in 1:dim(x_)[1]) {
  
  im &lt;- t(x_[image, , ])
  
  # Coffee cleaning function
  # detour the coffee beans and replace the invalid space by NA
  
  ## First layer ####
  
  ### First layer - left ####
  
  bw_diff &lt;- 50 # first threshold
  for (lindex in 1:dim(im)[1]) {
    
    # extract one line
    line &lt;- im[lindex, ]
    trash &lt;- replicate(n = length(line), TRUE)
    
    # find a differential
    for (position in 1:(length(line) - 1)) {
      first_value &lt;- line[position]
      second_value &lt;- line[position + 1]
      
      # presence of NAs?
      if (is.na(second_value - first_value)) {}
      
      else {
      
        if (sqrt((second_value - first_value)^2) &gt;= bw_diff) {
          trash &lt;- replace(x = trash, list = 1:(position + 1), values = FALSE)
          break
        } else {}
      }
    }
    
    line &lt;- replace(line, list = which(trash == FALSE), values = NA)
    im[lindex, ] &lt;- line
    
  }
  
  
  ### First layer - right ####
  
  bw_diff &lt;- 50 # first threshold
  for (lindex in 1:dim(im)[1]) {
    
    # extract one line
    line &lt;- im[lindex, ]
    trash &lt;- replicate(n = length(line), TRUE)
    
    # find a differential
    for (position in (length(line)):2) {
      first_value &lt;- line[position]
      second_value &lt;- line[position - 1]
      
      # presence of NAs?
      if (is.na(second_value - first_value)) {}
      
      else {
      
        if (sqrt((second_value - first_value)^2) &gt;= bw_diff) {
          
          # trash only if first value is not the black corner
          # otherwise, we are doing the second layer job.
          if (line[length(line)] &lt; 200) {
            trash &lt;- replace(x = trash, list = length(line):(position - 1), values = FALSE)
          } else {
            break
          }
          break
        } else {}
      }
    }
    
    line &lt;- replace(line, list = which(trash == FALSE), values = NA)
    im[lindex, ] &lt;- line
    
  }
  
  
  ## Side layer - topbottom ####
  
  # roughly cut the edges
  
  edge_left &lt;- 1:10
  edge_right &lt;- 90:100
  
  im[edge_left, ] &lt;- NA
  im[edge_right, ] &lt;- NA
  
  
  ## Second Layer ####
  
  ### Second layer - left ####
  
  bw_diff &lt;- 10 # second, sensitive threshold
  for (lindex in 1:dim(im)[1]) {
    
    # extract one line
    line &lt;- im[lindex, ]
    trash &lt;- replicate(n = length(line), TRUE)
    
    # find a differential
    for (position in 1:(length(line) - 1)) {
      
      first_value &lt;- line[position]
      second_value &lt;- line[position + 1]
      third_value &lt;- line[position + 2]
      fourth_value &lt;- line[position + 3]
      
      # presence of NAs?
      if (any(is.na(c(first_value, 
                      second_value, 
                      third_value,
                      fourth_value)))) {}
      
      else {
        
        if (sqrt((second_value - first_value)^2) &gt;= bw_diff &amp; 
            third_value &lt; 220 &amp;
            fourth_value &lt; 220) {
          trash &lt;- replace(x = trash, list = 1:(position + 1), values = FALSE)
          break
        } else {}
      }
    }
    
    line &lt;- replace(line, list = which(trash == FALSE), values = NA)
    im[lindex, ] &lt;- line
    
  }
  
  
  ### Second layer - right ####
  
  bw_diff &lt;- 10 # second, sensitive threshold
  for (lindex in 1:dim(im)[1]) {
    
    # extract one line
    line &lt;- im[lindex, ]
    trash &lt;- replicate(n = length(line), TRUE)
    
    # find a differential
    for (position in (length(line)):2) {
      
      first_value &lt;- line[position]
      second_value &lt;- line[position - 1]
      third_value &lt;- line[position - 2]
      fourth_value &lt;- line[position - 3]
      
      # presence of NAs?
      if (any(is.na(c(first_value, 
                      second_value,
                      third_value,
                      fourth_value)))) {}
      
      else {
        
        if (sqrt((second_value - first_value)^2) &gt;= bw_diff &amp;
            third_value &lt; 220 &amp;
            fourth_value &lt; 220) {
          trash &lt;- replace(x = trash, list = length(line):(position - 1), values = FALSE)
          break
        } else {}
      }
    }
    
    line &lt;- replace(line, list = which(trash == FALSE), values = NA)
    im[lindex, ] &lt;- line
    
  }
  
  ## Third layer - mustache ####
  
  for (lindex in 1:dim(im)[1]) {
    
    line &lt;- im[lindex, ]
    line_strip &lt;- line[!is.na(line)]
    
    if (length(line_strip) &gt;= 1) {
      
      ra &lt;- range(line_strip)
      if (ra[1] &gt; 170 &amp; ra[2] &gt; 170) {
        im[lindex, ] &lt;- replicate(100, NA)
      } else {}
      
    } else {}
    
  }
  
  cat(&quot;Bean N°&quot;, image, &quot;processed sucessfully! \n&quot;)
  x_clean[image, , ] &lt;- im
  
}</code></pre>
<p>The figure 2.5 shows the first eight cleaned images of each bean category.</p>
<pre class="r"><code># list the bean names for plot

list_for_plot &lt;- c(&quot;ded_1&quot;,
                   &quot;ded_2&quot;,
                   &quot;ded_3&quot;,
                   &quot;ded_4&quot;,
                   &quot;ded_5&quot;,
                   &quot;ded_6&quot;,
                   &quot;ded_7&quot;,
                   &quot;ded_8&quot;, 
                   &quot;egn_1&quot;,
                   &quot;egn_2&quot;,
                   &quot;egn_3&quot;,
                   &quot;egn_4&quot;,
                   &quot;egn_5&quot;,
                   &quot;egn_6&quot;,
                   &quot;egn_7&quot;,
                   &quot;egn_8&quot;)


index &lt;- clean_names %in% list_for_plot
data &lt;- (x_clean[index, , ])

# format the matrices into tidy data frames

gg_images &lt;- data.frame()
for (i in 1:dim(data)[1]) {
  
  image &lt;- as.data.frame(data[i, , ])
  colnames(image) &lt;- seq_len(ncol(image))
  image$y &lt;- seq_len(nrow(image))
  image &lt;- gather(image, &quot;x&quot;, &quot;value&quot;, -y)
  image$x &lt;- as.integer(image$x)
  image$image &lt;- list_for_plot[i]
  gg_images &lt;- rbind(gg_images, image)
  
}

# plot the data frames

ggplot(gg_images, aes(x = y, y = x, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = &quot;blue&quot;, high = &quot;orange&quot;, na.value = NA) +
  facet_wrap(image ~ ., ncol = 4) +
  
  theme_bw() +
  theme(panel.grid = element_blank())   +
  theme(aspect.ratio = 1) +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  ylab(&quot;&quot;) +
  xlab(&quot;&quot;)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-8"></span>
<img src="coffee_classification_files/figure-html/unnamed-chunk-8-1.png" alt="Overview of the first eight cleaned matrices of each category." width="90%" />
<p class="caption">
Figure 2.5: Overview of the first eight cleaned matrices of each category.
</p>
</div>
</div>
</div>
<div id="variable-extraction" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Variable extraction</h2>
<p>Now that our dataset contains bean-only images, we can start extracting the predictors.</p>
<div id="surface" class="section level3" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Surface</h3>
<p>We define the bean size as the surface of the image covered by the bean (size and surface are considered the same variable in this case).</p>
<p><span class="math display">\[
Surface_b = \sum_{i = 1}^{n} pixel_i \tag {2}
\]</span>
<span class="math display">\[
\forall \ pixel \ \in \ bean
\]</span></p>
<p>The surfaces were extracted from the cleaned images by flattening the matrices and removing they NA values. The results were then stored into a dataframe. The figures 2.6 and 2.7 show the surfaces distributions for the two bean types. Comparing these surfaces distribution, we observe that the DED beans feature a higher average surface and higher variability compared to the EGN beans, as summarized in table 2.1.</p>
<pre class="r"><code># surface

surface &lt;- data.frame()

for (i in 1:dim(x_clean)[1]) {
  
  bean &lt;- x_clean[i, , ]
  bean &lt;- c(bean)
  bean &lt;- bean[!is.na(bean)]
  
  ds &lt;- data.frame(surface = length(bean), 
                   name = clean_names[i],
                   type = outcomes[i])
  
  surface &lt;- rbind(surface, ds)
  
}

ggplot(data = surface, aes(y = surface, x = type, col = type)) +
  geom_jitter(width = 0.1, alpha = 0.6) +
  theme_bw() +
  labs(y = &quot;Surface (pixel count)&quot;, x = &quot;Coffee type&quot;)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-9"></span>
<img src="coffee_classification_files/figure-html/unnamed-chunk-9-1.png" alt="Bean surfaces by coffee type." width="80%" />
<p class="caption">
Figure 2.6: Bean surfaces by coffee type.
</p>
</div>
<pre class="r"><code># surface distribution

ggplot(data = surface, aes(x = surface, col = type)) + 
  geom_density() +
  theme_bw() +
  labs(y = &quot;Density&quot;, x = &quot;Surface (pixel count)&quot;)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-10"></span>
<img src="coffee_classification_files/figure-html/unnamed-chunk-10-1.png" alt="Density of bean surfaces by coffee type." width="80%" />
<p class="caption">
Figure 2.7: Density of bean surfaces by coffee type.
</p>
</div>
<pre class="r"><code># surface summary

tbl &lt;- surface %&gt;% group_by(type) %&gt;%
  summarise(mean = mean(surface),
            std = sd(surface)) %&gt;%
  mutate_if(is.numeric, round, digits = 1)

knitr::kable(tbl,
             format = &quot;html&quot;,
             row.names = FALSE,
             caption = &quot;Surface mean and standard deviation by bean type.&quot;,
             col.names = c(&quot;Coffee type&quot;, &quot;Mean&quot;, &quot;Std&quot;)) %&gt;%
  kable_classic(full_width = F,
                font_size = 14,
                html_font = &quot;sans-serif&quot;)</code></pre>
<table class=" lightable-classic" style="font-size: 14px; font-family: sans-serif; width: auto !important; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-11">Table 2.1: </span>Surface mean and standard deviation by bean type.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Coffee type
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Std
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
ded
</td>
<td style="text-align:right;">
2205.3
</td>
<td style="text-align:right;">
328.2
</td>
</tr>
<tr>
<td style="text-align:left;">
egn
</td>
<td style="text-align:right;">
1767.7
</td>
<td style="text-align:right;">
294.4
</td>
</tr>
</tbody>
</table>
</div>
<div id="color" class="section level3" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Color</h3>
<p>We define the bean color as the average of its intensity values.</p>
<p><span class="math display">\[
Color_b = \frac{1}{n}  \sum_{i = 1}^{n} pixel \ intensity_i \tag {3}
\]</span></p>
<p><span class="math display">\[
\forall \ pixel \ \in \ bean
\]</span></p>
<p>Similarly to the surface predictor, the color was extracted by flattening the matrices, removing they NA values, and storing the results into a dataframe. The figures 2.8 and 2.9 show the color distributions for the two bean types. We observe that the DED beans feature a lower color intensity compared to the EGN beans, as summarized in table 2.2.</p>
<pre class="r"><code># color

colors &lt;- data.frame()

for (i in 1:dim(x_clean)[1]) {
  
  bean &lt;- x_clean[i, , ]
  bean &lt;- bean[!is.na(bean)]
  
  ds &lt;- data.frame(name = clean_names[i],
                   type = outcomes[i],
                   intensity = mean(bean))
  
  colors &lt;- rbind(colors, ds)
}

ggplot(data = colors, aes(y = intensity, x = type, col = type)) + 
  geom_jitter(width = 0.1, alpha = 0.6) +
  theme_bw() +
  labs(y = &quot;Color intensity (0 to 255)&quot;, x = &quot;Coffee type&quot;)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-12"></span>
<img src="coffee_classification_files/figure-html/unnamed-chunk-12-1.png" alt="Bean colors by coffee type" width="80%" />
<p class="caption">
Figure 2.8: Bean colors by coffee type
</p>
</div>
<pre class="r"><code># color distribution

ggplot(data = colors, aes(x = intensity, col = type)) + 
  geom_density() +
  theme_bw() +
  labs(y = &quot;Density&quot;, x = &quot;Color intensity (0 to 255)&quot;)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-13"></span>
<img src="coffee_classification_files/figure-html/unnamed-chunk-13-1.png" alt="Density of bean colors by coffee type." width="80%" />
<p class="caption">
Figure 2.9: Density of bean colors by coffee type.
</p>
</div>
<pre class="r"><code># color summary

tbl &lt;- colors %&gt;% group_by(type) %&gt;%
  summarise(mean = mean(intensity),
            std = sd(intensity)) %&gt;%
  mutate_if(is.numeric, round, digits = 1)

knitr::kable(tbl,
             format = &quot;html&quot;,
             row.names = FALSE,
             caption = &quot;Color mean and standard deviation by bean type.&quot;,
             col.names = c(&quot;Coffee type&quot;, &quot;Mean&quot;, &quot;Std&quot;)) %&gt;%
  kable_classic(full_width = F,
                font_size = 14,
                lightable_options = &quot;basic&quot;,
                html_font = &quot;sans-serif&quot;)</code></pre>
<table class=" lightable-classic" style="font-size: 14px; font-family: sans-serif; width: auto !important; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-14">Table 2.2: </span>Color mean and standard deviation by bean type.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Coffee type
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Std
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
ded
</td>
<td style="text-align:right;">
106.0
</td>
<td style="text-align:right;">
13.0
</td>
</tr>
<tr>
<td style="text-align:left;">
egn
</td>
<td style="text-align:right;">
129.3
</td>
<td style="text-align:right;">
9.4
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="classification" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Classification</h1>
<p>Luckily, we are able to observe a difference in surface between the two coffee types, and an even greater difference in color! Our processed images capture that the DED beans are indeed wider and darker than the EGN beans. Plotting the predictors one as a function of the other shows a trend that defined the classification method selected (c.f. Figure 3.1).</p>
<pre class="r"><code># surface vs color

surfcol &lt;- full_join(surface, colors, by = c(&quot;name&quot;, &quot;type&quot;))

ggplot(surfcol, aes(y = surface, x = intensity, col = type)) +
  geom_point() +
  theme_bw() +
  labs(y = &quot;Surface (pixel count)&quot;, x = &quot;Color intensity (0 to 255)&quot;)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-15"></span>
<img src="coffee_classification_files/figure-html/unnamed-chunk-15-1.png" alt="Bean surfaces as a function of they average color internsity." width="80%" />
<p class="caption">
Figure 3.1: Bean surfaces as a function of they average color internsity.
</p>
</div>
<div id="logistic-regression" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Logistic regression</h2>
<p>We initially intended to build and train a neural network on the images using <em>Tensorflow</em> which is available in R through the <em>Keras</em> package. However, based on this clear trend, we simply established a logistic regression to segregate the clusters.</p>
<p>As our conditional probability is defined as</p>
<p><span class="math display">\[
\hat{p}(x_1, x_2) = Pr(Y = k \ | \ X_1 = x_1, X_2 = x_2) \tag{4}
\]</span></p>
<p>The model was built by fitting a boundary defined by the following decision rule. A linear boundary will segregate the two clusters based on a <span class="math inline">\(0.5\)</span> conditional probability of observing a DED bean as a function of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>.</p>
<p><span class="math display">\[
\hat{p}(x_1, x_2) = \hat{\beta}_0 + \hat{\beta}_1x_1 + \hat{\beta}_2x_2 = 0.5 \tag{5}
\]</span></p>
<p><span class="math inline">\(x_2\)</span> can be expressed as a function of <span class="math inline">\(x_1\)</span> when plotting the regression line, which gives the following equations.</p>
<p><span class="math display">\[
x_2 = \frac{0.5 - \hat{\beta}_0 - \hat{\beta}_1x_1}{\hat{\beta}_2} \tag{6}
\]</span></p>
<p><span class="math display">\[
SurfaceBoundary = \frac{0.5 - \hat{\beta}_0 - \hat{\beta}_1Intensity}{\hat{\beta}_2} \tag{7}
\]</span></p>
<p>Before fitting this model, we created two data partitions to avoid overfitting. Those partitions were created using the function <em>createDataPartition</em> from the <em>caret</em> package. The DED type was defined as the outcome <span class="math inline">\(Y = 1\)</span> and the EGN type as <span class="math inline">\(Y = 0\)</span>. We first trained our model on a labeled data partition (<em>train_set</em>) and then applied it on an unlabeled data partition (<em>test_set</em>) to make predictions (<em>y_hat</em>). The coefficients <span class="math inline">\(\hat{\beta}_0\)</span>, <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_2\)</span> were retrieved from the model and used to draw the segregation line defined in equation 7.</p>
<pre class="r"><code># create a data partition

test_index &lt;- createDataPartition(y = surfcol$type, times = 1, p = 0.5, list = FALSE)

test_set &lt;- surfcol[test_index, ]
train_set &lt;- surfcol[-test_index, ]

# build the model on the train set

fit &lt;- train_set %&gt;%
  mutate(y = ifelse(type == &quot;ded&quot;, 1, 0)) %&gt;%
  lm(y ~ intensity + surface, data = .)

# assess the model on the test set

p_hat &lt;- predict(fit, newdata = test_set)
y_hat &lt;- factor(ifelse(p_hat &gt; 0.5, &quot;ded&quot;, &quot;egn&quot;))

# plot a segregation line

surfcol$prob &lt;- p_hat

beta_0 &lt;- fit$coefficients[1]
beta_1 &lt;- fit$coefficients[2]
beta_2 &lt;- fit$coefficients[3]

surfcol &lt;- surfcol %&gt;% mutate(y_05 = (0.5 - beta_0 - beta_1*intensity)/beta_2)

ggplot(surfcol, aes(y = surface, x = intensity, col = type)) +
  geom_point() +
  geom_line(aes(y = y_05,
                x = intensity,
                linetype = &quot;p(Intensity, Surface) = 0.5&quot;), 
            col = &quot;black&quot;) +
  scale_linetype_manual(name = &quot;boundary&quot;,
                        values = c(2),
                        guide = guide_legend(override.aes = list(color = c(&quot;black&quot;)))) +
  theme_bw() +
  labs(y = &quot;Surface (pixel count)&quot;, x = &quot;Color intensity (0 to 255)&quot;)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-16"></span>
<img src="coffee_classification_files/figure-html/unnamed-chunk-16-1.png" alt="Bean surfaces as a function of they average color internsity." width="80%" />
<p class="caption">
Figure 3.2: Bean surfaces as a function of they average color internsity.
</p>
</div>
<p>The regression line performed on the complete dataset shows an efficient segregation between the two bean types. At a first glance, we observe about only a few false predictions for the two bean types (c.f. Figure 3.2).</p>
</div>
<div id="accuracy-scores" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Accuracy scores</h2>
<p>The model accuracy was assessed by comparing the predictions (<em>y_hat</em>) and the actual outcomes retrieved from the unlabeled data partition (<em>test_set</em>) using the function <em>confusionMatrix</em> from the <em>caret</em> package (c.f. Table 3.1). Based on this comparison, we selected the following indicators to assess the model quality: Sensitivity, specificity, balanced accuracy and the <span class="math inline">\(F_1\)</span> score (c.f. Table 3.2).</p>
<p><span class="math display">\[
Sensitivity (TPR) = \frac{True_{DED}}{True_{DED} + False_{EGN}} = Pr(\hat{Y} = DED \ | \ Y = DED)\tag{8}
\]</span></p>
<p><span class="math display">\[
Specificity (TNR) = \frac{True_{EGN}}{True_{EGN} + False_{DED}} = Pr(\hat{Y} = EGN \ | \ Y = EGN) \tag{9}
\]</span></p>
<p><span class="math display">\[
Specificity (PPV) = \frac{True_{DED}}{True_{DED} + False_{DED}} = Pr(Y = DED \ | \ \hat{Y} = DED) \tag{10}
\]</span></p>
<p><span class="math display">\[
BalancedAccuracy = \frac{sensitivity+specificity}{2} \tag{11}
\]</span>
<span class="math display">\[
F_1Score = \frac{1}{\frac{\beta^2}{1+\beta^2} \frac{1}{TPR} + \frac{1}{1+\beta^2} \frac{1}{PPV}} \tag{12}
\]</span></p>
<p>In our case, since the sensitivity is as important as the specificity, the <span class="math inline">\(\beta\)</span> parameter of the <span class="math inline">\(F_1Score\)</span> is kept to its default value of 1.</p>
<pre class="r"><code># assess the model accuracy: confusion matrix

cm &lt;- confusionMatrix(data = y_hat, reference = factor(test_set$type))
confmat &lt;- cm$table

knitr::kable(confmat,
             format = &quot;html&quot;,
             row.names = TRUE, 
             caption = &quot;Confusion matrix (actual versus predicted)&quot;, 
             col.names = c(&quot;Actual DED (Y = 1)&quot;, &quot;Actual EGN (Y = 0)&quot;)) %&gt;%
  kable_classic(full_width = F,
                font_size = 14,
                lightable_options = &quot;basic&quot;,
                html_font = &quot;sans-serif&quot;)</code></pre>
<table class=" lightable-classic" style="font-size: 14px; font-family: sans-serif; width: auto !important; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-17">Table 3.1: </span>Confusion matrix (actual versus predicted)
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Actual DED (Y = 1)
</th>
<th style="text-align:right;">
Actual EGN (Y = 0)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
ded
</td>
<td style="text-align:right;">
49
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
egn
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
50
</td>
</tr>
</tbody>
</table>
<pre class="r"><code># assess the model accuracy: scores

tbl &lt;- as.data.frame(cm$byClass) %&gt;%
  mutate_if(is.numeric, round, digits = 2)

knitr::kable(tbl,
             format = &quot;html&quot;,
             row.names = TRUE, 
             caption = &quot;Accuracy Scores&quot;, 
             col.names = c(&quot;Scores&quot;)) %&gt;%
  kable_classic(full_width = F,
                font_size = 14,
                lightable_options = &quot;basic&quot;,
                html_font = &quot;sans-serif&quot;)</code></pre>
<table class=" lightable-classic" style="font-size: 14px; font-family: sans-serif; width: auto !important; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-18">Table 3.2: </span>Accuracy Scores
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Scores
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Sensitivity
</td>
<td style="text-align:right;">
0.98
</td>
</tr>
<tr>
<td style="text-align:left;">
Specificity
</td>
<td style="text-align:right;">
1.00
</td>
</tr>
<tr>
<td style="text-align:left;">
Pos Pred Value
</td>
<td style="text-align:right;">
1.00
</td>
</tr>
<tr>
<td style="text-align:left;">
Neg Pred Value
</td>
<td style="text-align:right;">
0.98
</td>
</tr>
<tr>
<td style="text-align:left;">
Precision
</td>
<td style="text-align:right;">
1.00
</td>
</tr>
<tr>
<td style="text-align:left;">
Recall
</td>
<td style="text-align:right;">
0.98
</td>
</tr>
<tr>
<td style="text-align:left;">
F1
</td>
<td style="text-align:right;">
0.99
</td>
</tr>
<tr>
<td style="text-align:left;">
Prevalence
</td>
<td style="text-align:right;">
0.50
</td>
</tr>
<tr>
<td style="text-align:left;">
Detection Rate
</td>
<td style="text-align:right;">
0.49
</td>
</tr>
<tr>
<td style="text-align:left;">
Detection Prevalence
</td>
<td style="text-align:right;">
0.49
</td>
</tr>
<tr>
<td style="text-align:left;">
Balanced Accuracy
</td>
<td style="text-align:right;">
0.99
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="conclusion" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Conclusion</h1>
<p>To sum up this project, an <span class="math inline">\(F_1Score\)</span> of 0.99 as presented in table 3.2 is beyond expectations for this first iteration, and perfectly illustrate the capability of our model to segregate the two bean types selected. An interesting next step would be to (i) increase the number of predictors (e.g. reflectance, length over width ratio, RGB color channels, etc.) and perform different dimension reduction analysis (although it doesn’t seem necessary considering the quality of our current model) (ii) increase the number of bean type and investigate if a clustering analysis would provide the same level of accuracy.</p>
</div>
<div id="ressources" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Ressources</h1>
<ol style="list-style-type: decimal">
<li><em>Basic image classification</em>, <a href="https://tensorflow.rstudio.com/tutorials/keras/classification" class="uri">https://tensorflow.rstudio.com/tutorials/keras/classification</a></li>
<li>R.Irizarry, <em>Introduction to data science</em>, CRC Press, 2020, <a href="https://rafalab.dfci.harvard.edu/dsbook/" class="uri">https://rafalab.dfci.harvard.edu/dsbook/</a></li>
<li><em>RVision - A computer vision library for R</em>, <a href="https://swarm-lab.github.io/Rvision/" class="uri">https://swarm-lab.github.io/Rvision/</a></li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
